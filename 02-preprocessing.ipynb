{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Preprocessing Pipeline: Diabetes Prediction\n",
    "\n",
    "\n",
    "\n",
    " **Extension of Nguyen & Zhang (2025)**\n",
    "\n",
    "\n",
    "\n",
    " ## Notebook Structure\n",
    "\n",
    " 0. Forensics: Understanding the Paper's Approach\n",
    "\n",
    " 1. Load Data\n",
    "\n",
    " 2. Train/Test Split\n",
    "\n",
    " 3. Feature Engineering\n",
    "\n",
    " 4. Class Balancing (SMOTE vs Paper's Undersampling)\n",
    "\n",
    " 5. Feature Scaling\n",
    "\n",
    " 6. Save Preprocessed Data\n",
    "\n",
    " 7. Data Leakage Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 0. Forensics: Understanding the Paper's Approach\n",
    "\n",
    "\n",
    "\n",
    " Before preprocessing, we reverse-engineer what Nguyen & Zhang (2025) did\n",
    "\n",
    " to their 50-50 balanced dataset. This informs our extension strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FORENSICS: PAPER'S DATASET vs ORIGINAL\n",
      "======================================================================\n",
      "Paper's balanced:   70,692 samples\n",
      "Original imbalanced: 253,680 samples\n",
      "Samples discarded:   182,988\n"
     ]
    }
   ],
   "source": [
    "# Load both datasets for comparison\n",
    "df_balanced = pd.read_csv('diabetes_binary_5050split_health_indicators_BRFSS2015.csv')\n",
    "df_imbalanced = pd.read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FORENSICS: PAPER'S DATASET vs ORIGINAL\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Paper's balanced:   {df_balanced.shape[0]:,} samples\")\n",
    "print(f\"Original imbalanced: {df_imbalanced.shape[0]:,} samples\")\n",
    "print(f\"Samples discarded:   {df_imbalanced.shape[0] - df_balanced.shape[0]:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "CHECK 1: CLASS DISTRIBUTION\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Paper's balanced dataset:\n",
      "Diabetes_binary\n",
      "0.0    35346\n",
      "1.0    35346\n",
      "Name: count, dtype: int64\n",
      "Ratio: 1.00:1\n",
      "\n",
      "Original imbalanced dataset:\n",
      "Diabetes_binary\n",
      "0.0    218334\n",
      "1.0     35346\n",
      "Name: count, dtype: int64\n",
      "Ratio: 6.18:1\n"
     ]
    }
   ],
   "source": [
    "# Check 1: Class distribution\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"CHECK 1: CLASS DISTRIBUTION\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\\nPaper's balanced dataset:\")\n",
    "print(df_balanced['Diabetes_binary'].value_counts())\n",
    "print(f\"Ratio: {df_balanced['Diabetes_binary'].value_counts()[0] / df_balanced['Diabetes_binary'].value_counts()[1]:.2f}:1\")\n",
    "\n",
    "print(\"\\nOriginal imbalanced dataset:\")\n",
    "print(df_imbalanced['Diabetes_binary'].value_counts())\n",
    "print(f\"Ratio: {df_imbalanced['Diabetes_binary'].value_counts()[0] / df_imbalanced['Diabetes_binary'].value_counts()[1]:.2f}:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "CHECK 2: SCALING DETECTION\n",
      "----------------------------------------------------------------------\n",
      "StandardScaler applied: NO\n",
      "MinMaxScaler applied: NO\n",
      "Raw values (no scaling): YES\n"
     ]
    }
   ],
   "source": [
    "# Check 2: Scaling detection\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"CHECK 2: SCALING DETECTION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "features = [c for c in df_balanced.columns if c != 'Diabetes_binary']\n",
    "stats_balanced = df_balanced[features].agg(['mean', 'std', 'min', 'max']).T\n",
    "stats_imbalanced = df_imbalanced[features].agg(['min', 'max']).T\n",
    "\n",
    "# StandardScaler check\n",
    "scaled = stats_balanced['mean'].abs().mean() < 0.1 and (stats_balanced['std'] - 1).abs().mean() < 0.1\n",
    "print(f\"StandardScaler applied: {'YES' if scaled else 'NO'}\")\n",
    "\n",
    "# MinMaxScaler check  \n",
    "minmax = (stats_balanced['min'] == 0).all() and (stats_balanced['max'] == 1).all()\n",
    "print(f\"MinMaxScaler applied: {'YES' if minmax else 'NO'}\")\n",
    "\n",
    "# Raw values check\n",
    "comparison = pd.DataFrame({\n",
    "    'bal_min': stats_balanced['min'],\n",
    "    'bal_max': stats_balanced['max'],\n",
    "    'imbal_min': stats_imbalanced['min'],\n",
    "    'imbal_max': stats_imbalanced['max']\n",
    "})\n",
    "comparison['min_match'] = comparison['bal_min'] == comparison['imbal_min']\n",
    "comparison['max_match'] = comparison['bal_max'] == comparison['imbal_max']\n",
    "raw = comparison['min_match'].all() and comparison['max_match'].all()\n",
    "print(f\"Raw values (no scaling): {'YES' if raw else 'NO'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "CHECK 3: BALANCING METHOD DETECTION\n",
      "----------------------------------------------------------------------\n",
      "Diabetic samples in balanced: 35,097\n",
      "Diabetic samples in imbalanced: 35,097\n",
      "Overlap: 35,097 (100.0%)\n",
      "\n",
      "→ Method: RANDOM UNDERSAMPLING\n",
      "  (All diabetic cases retained, non-diabetic randomly sampled)\n"
     ]
    }
   ],
   "source": [
    "# Check 3: Balancing method detection\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"CHECK 3: BALANCING METHOD DETECTION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Check if balanced data is a subset (undersampling) or has new samples (SMOTE)\n",
    "diabetic_balanced = df_balanced[df_balanced['Diabetes_binary'] == 1].drop('Diabetes_binary', axis=1)\n",
    "diabetic_imbalanced = df_imbalanced[df_imbalanced['Diabetes_binary'] == 1].drop('Diabetes_binary', axis=1)\n",
    "\n",
    "bal_tuples = set(diabetic_balanced.apply(tuple, axis=1))\n",
    "imbal_tuples = set(diabetic_imbalanced.apply(tuple, axis=1))\n",
    "overlap = len(bal_tuples.intersection(imbal_tuples))\n",
    "\n",
    "print(f\"Diabetic samples in balanced: {len(bal_tuples):,}\")\n",
    "print(f\"Diabetic samples in imbalanced: {len(imbal_tuples):,}\")\n",
    "print(f\"Overlap: {overlap:,} ({overlap/len(bal_tuples)*100:.1f}%)\")\n",
    "\n",
    "if overlap / len(bal_tuples) > 0.95:\n",
    "    print(\"\\n→ Method: RANDOM UNDERSAMPLING\")\n",
    "    print(\"  (All diabetic cases retained, non-diabetic randomly sampled)\")\n",
    "else:\n",
    "    print(\"\\n→ Method: Unknown (possibly SMOTE or augmentation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FORENSICS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "PAPER'S APPROACH:\n",
      "  - Scaling: None (raw values)\n",
      "  - Balancing: Random undersampling of majority class\n",
      "  - Data discarded: ~183,000 non-diabetic samples\n",
      "  - Test distribution: Balanced 50/50\n",
      "\n",
      "OUR EXTENSION:\n",
      "  - Scaling: StandardScaler on ordinal + numeric features\n",
      "  - Balancing: SMOTE oversampling (preserves all original data)\n",
      "  - Additional: SMOTE-Tomek hybrid, class_weight comparison\n",
      "  - Test distribution: Realistic 86/14 imbalanced\n",
      "\n",
      "WHY THIS MATTERS:\n",
      "  - Our SMOTE approach is a genuine methodological extension\n",
      "  - Different test distributions will affect F1 scores (not model quality)\n",
      "  - Fair comparison requires ROC-AUC (threshold-independent)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Forensics summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FORENSICS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "PAPER'S APPROACH:\n",
    "  - Scaling: None (raw values)\n",
    "  - Balancing: Random undersampling of majority class\n",
    "  - Data discarded: ~183,000 non-diabetic samples\n",
    "  - Test distribution: Balanced 50/50\n",
    "\n",
    "OUR EXTENSION:\n",
    "  - Scaling: StandardScaler on ordinal + numeric features\n",
    "  - Balancing: SMOTE oversampling (preserves all original data)\n",
    "  - Additional: SMOTE-Tomek hybrid, class_weight comparison\n",
    "  - Test distribution: Realistic 86/14 imbalanced\n",
    "\n",
    "WHY THIS MATTERS:\n",
    "  - Our SMOTE approach is a genuine methodological extension\n",
    "  - Different test distributions will affect F1 scores (not model quality)\n",
    "  - Fair comparison requires ROC-AUC (threshold-independent)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREPROCESSING: OUR APPROACH\n",
      "======================================================================\n",
      "\n",
      "Original data shape: (253680, 21)\n",
      "Class distribution:\n",
      "Diabetes_binary\n",
      "0.0    0.861\n",
      "1.0    0.139\n",
      "Name: proportion, dtype: float64\n",
      "Imbalance ratio: 6.2:1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREPROCESSING: OUR APPROACH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df = pd.read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')\n",
    "X = df.drop('Diabetes_binary', axis=1)\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "print(f\"\\nOriginal data shape: {X.shape}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts(normalize=True).round(3)}\")\n",
    "print(f\"Imbalance ratio: {y.value_counts()[0] / y.value_counts()[1]:.1f}:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature groups\n",
    "BINARY_FEATURES = [\n",
    "    'HighBP', 'HighChol', 'CholCheck', 'Smoker', 'Stroke',\n",
    "    'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
    "    'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'DiffWalk', 'Sex'\n",
    "]\n",
    "ORDINAL_FEATURES = ['GenHlth', 'Age', 'Education', 'Income']\n",
    "NUMERIC_FEATURES = ['BMI', 'MentHlth', 'PhysHlth']\n",
    "FEATURES_TO_SCALE = ORDINAL_FEATURES + NUMERIC_FEATURES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAIN/TEST SPLIT\n",
      "======================================================================\n",
      "Training set: 177,576 samples\n",
      "Test set: 76,104 samples\n",
      "Train class distribution:\n",
      "Diabetes_binary\n",
      "0.0    0.861\n",
      "1.0    0.139\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Train class distribution:\\n{y_train.value_counts(normalize=True).round(3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Feature Engineering\n",
    "\n",
    "\n",
    "\n",
    " Based on EDA findings:\n",
    "\n",
    " - BMI is right-skewed → log transform\n",
    "\n",
    " - MentHlth/PhysHlth are zero-inflated → binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df, log_transform_bmi=True, bin_health_days=True):\n",
    "    \"\"\"Apply feature engineering based on EDA insights.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if log_transform_bmi:\n",
    "        df['BMI_log'] = np.log(df['BMI'])\n",
    "    \n",
    "    if bin_health_days:\n",
    "        df['MentHlth_binned'] = pd.cut(\n",
    "            df['MentHlth'], bins=[-1, 0, 10, 30], labels=[0, 1, 2]\n",
    "        ).astype(int)\n",
    "        df['PhysHlth_binned'] = pd.cut(\n",
    "            df['PhysHlth'], bins=[-1, 0, 10, 30], labels=[0, 1, 2]\n",
    "        ).astype(int)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING\n",
      "======================================================================\n",
      "New features added: ['BMI_log', 'MentHlth_binned', 'PhysHlth_binned']\n",
      "Total features: 24\n"
     ]
    }
   ],
   "source": [
    "APPLY_FEATURE_ENGINEERING = True\n",
    "\n",
    "if APPLY_FEATURE_ENGINEERING:\n",
    "    X_train = engineer_features(X_train)\n",
    "    X_test = engineer_features(X_test)\n",
    "    new_features = [c for c in X_train.columns if c not in X.columns]\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"New features added: {new_features}\")\n",
    "    print(f\"Total features: {X_train.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Class Balancing\n",
    "\n",
    "\n",
    "\n",
    " We create multiple versions for comparison:\n",
    "\n",
    " - **SMOTE**: Synthetic oversampling of minority class\n",
    "\n",
    " - **SMOTE-Tomek**: SMOTE + remove overlapping samples\n",
    "\n",
    " - **None**: Use class_weight in models instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(X_train, y_train, method='smote', random_state=42):\n",
    "    \"\"\"Balance training data using specified method.\"\"\"\n",
    "    if method == 'none':\n",
    "        return X_train, y_train\n",
    "    elif method == 'smote':\n",
    "        sampler = SMOTE(random_state=random_state)\n",
    "    elif method == 'smote_tomek':\n",
    "        sampler = SMOTETomek(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    X_balanced, y_balanced = sampler.fit_resample(X_train, y_train)\n",
    "    return X_balanced, y_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLASS BALANCING\n",
      "======================================================================\n",
      "\n",
      "No balancing (class_weight):\n",
      "  Samples: 177,576\n",
      "  Class 0: 152,834, Class 1: 24,742\n",
      "\n",
      "SMOTE:\n",
      "  Samples: 305,668 (was 177,576)\n",
      "  Class 0: 152,834, Class 1: 152,834\n",
      "\n",
      "SMOTE-Tomek:\n",
      "  Samples: 305,466\n",
      "  Class 0: 152,733, Class 1: 152,733\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLASS BALANCING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# No balancing (for class_weight approach)\n",
    "X_train_none, y_train_none = balance_data(X_train, y_train, method='none')\n",
    "print(f\"\\nNo balancing (class_weight):\")\n",
    "print(f\"  Samples: {len(X_train_none):,}\")\n",
    "print(f\"  Class 0: {(y_train_none == 0).sum():,}, Class 1: {(y_train_none == 1).sum():,}\")\n",
    "\n",
    "# SMOTE\n",
    "X_train_smote, y_train_smote = balance_data(X_train, y_train, method='smote')\n",
    "print(f\"\\nSMOTE:\")\n",
    "print(f\"  Samples: {len(X_train_smote):,} (was {len(X_train):,})\")\n",
    "print(f\"  Class 0: {(y_train_smote == 0).sum():,}, Class 1: {(y_train_smote == 1).sum():,}\")\n",
    "\n",
    "# SMOTE-Tomek\n",
    "X_train_smote_tomek, y_train_smote_tomek = balance_data(X_train, y_train, method='smote_tomek')\n",
    "print(f\"\\nSMOTE-Tomek:\")\n",
    "print(f\"  Samples: {len(X_train_smote_tomek):,}\")\n",
    "print(f\"  Class 0: {(y_train_smote_tomek == 0).sum():,}, Class 1: {(y_train_smote_tomek == 1).sum():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Feature Scaling\n",
    "\n",
    "\n",
    "\n",
    " Unlike the paper (no scaling), we apply StandardScaler to ordinal and numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train, X_test, features_to_scale):\n",
    "    \"\"\"Scale specified features. Fit on train, transform both.\"\"\"\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "    X_test_scaled[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE SCALING\n",
      "======================================================================\n",
      "Scaling features: ['GenHlth', 'Age', 'Education', 'Income', 'BMI', 'MentHlth', 'PhysHlth']\n",
      "\n",
      "Scaled feature stats (SMOTE training set):\n",
      "      GenHlth  Age  Education  Income  BMI  MentHlth  PhysHlth\n",
      "mean      0.0 -0.0       -0.0     0.0  0.0       0.0      -0.0\n",
      "std       1.0  1.0        1.0     1.0  1.0       1.0       1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Scaling features: {FEATURES_TO_SCALE}\")\n",
    "\n",
    "# Scale each version\n",
    "X_train_smote_scaled, X_test_scaled, scaler = scale_features(\n",
    "    X_train_smote, X_test, FEATURES_TO_SCALE\n",
    ")\n",
    "X_train_smote_tomek_scaled, _, _ = scale_features(\n",
    "    X_train_smote_tomek, X_test, FEATURES_TO_SCALE\n",
    ")\n",
    "X_train_unbalanced_scaled, _, _ = scale_features(\n",
    "    X_train, X_test, FEATURES_TO_SCALE\n",
    ")\n",
    "\n",
    "print(\"\\nScaled feature stats (SMOTE training set):\")\n",
    "print(X_train_smote_scaled[FEATURES_TO_SCALE].describe().round(2).loc[['mean', 'std']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREPROCESSED DATA SUMMARY\n",
      "======================================================================\n",
      "\n",
      "smote:\n",
      "  SMOTE oversampling + StandardScaler\n",
      "  Train: 305,668 samples, 24 features\n",
      "  Test: 76,104 samples\n",
      "\n",
      "smote_tomek:\n",
      "  SMOTE-Tomek hybrid + StandardScaler\n",
      "  Train: 305,466 samples, 24 features\n",
      "  Test: 76,104 samples\n",
      "\n",
      "class_weight:\n",
      "  No resampling (use class_weight=\"balanced\")\n",
      "  Train: 177,576 samples, 24 features\n",
      "  Test: 76,104 samples\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = {\n",
    "    'smote': {\n",
    "        'X_train': X_train_smote_scaled,\n",
    "        'y_train': y_train_smote,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_test': y_test,\n",
    "        'description': 'SMOTE oversampling + StandardScaler'\n",
    "    },\n",
    "    'smote_tomek': {\n",
    "        'X_train': X_train_smote_tomek_scaled,\n",
    "        'y_train': y_train_smote_tomek,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_test': y_test,\n",
    "        'description': 'SMOTE-Tomek hybrid + StandardScaler'\n",
    "    },\n",
    "    'class_weight': {\n",
    "        'X_train': X_train_unbalanced_scaled,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_test': y_test,\n",
    "        'description': 'No resampling (use class_weight=\"balanced\")'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREPROCESSED DATA SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "for name, data in preprocessed_data.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  {data['description']}\")\n",
    "    print(f\"  Train: {data['X_train'].shape[0]:,} samples, {data['X_train'].shape[1]} features\")\n",
    "    print(f\"  Test: {data['X_test'].shape[0]:,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Data saved to preprocessed_data.pkl\n",
      "✓ Scaler saved to scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"\\n✓ Data saved to preprocessed_data.pkl\")\n",
    "print(\"✓ Scaler saved to scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Data Leakage Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA LEAKAGE CHECK\n",
      "======================================================================\n",
      "✓ Test set size unchanged: True\n",
      "✓ Test class distribution preserved:\n",
      "    {0.0: 0.861, 1.0: 0.139}\n",
      "✓ Training scaled means near 0: True\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "SUMMARY OF DIFFERENCES FROM PAPER:\n",
      "\n",
      "| Aspect            | Paper             | Our Extension           |\n",
      "|-------------------|-------------------|-------------------------|\n",
      "| Scaling           | None              | StandardScaler          |\n",
      "| Balancing         | Undersampling     | SMOTE / SMOTE-Tomek     |\n",
      "| Data preserved    | 70,692            | 253,680 (all)           |\n",
      "| Test distribution | 50/50 balanced    | 86/14 realistic         |\n",
      "| Feature eng.      | None              | BMI_log, binned health  |\n",
      "\n",
      "→ Proceed to 03_modeling.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA LEAKAGE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test set size unchanged\n",
    "print(f\"✓ Test set size unchanged: {len(X_test) == len(X_test_scaled)}\")\n",
    "\n",
    "# Test class distribution unchanged\n",
    "print(f\"✓ Test class distribution preserved:\")\n",
    "print(f\"    {y_test.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "\n",
    "# Scaling fit only on training\n",
    "train_means = X_train_smote_scaled[FEATURES_TO_SCALE].mean()\n",
    "print(f\"✓ Training scaled means near 0: {(train_means.abs() < 0.01).all()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "SUMMARY OF DIFFERENCES FROM PAPER:\n",
    "\n",
    "| Aspect            | Paper             | Our Extension           |\n",
    "|-------------------|-------------------|-------------------------|\n",
    "| Scaling           | None              | StandardScaler          |\n",
    "| Balancing         | Undersampling     | SMOTE / SMOTE-Tomek     |\n",
    "| Data preserved    | 70,692            | 253,680 (all)           |\n",
    "| Test distribution | 50/50 balanced    | 86/14 realistic         |\n",
    "| Feature eng.      | None              | BMI_log, binned health  |\n",
    "\n",
    "→ Proceed to 03_modeling.py\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
